{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/13 11:05:49 WARN Utils: Your hostname, codespaces-56288b resolves to a loopback address: 127.0.0.1; using 10.0.2.249 instead (on interface eth0)\n",
      "25/03/13 11:05:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/13 11:05:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/13 11:06:08 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+--------+\n",
      "|  id|   name|age|    city|\n",
      "+----+-------+---+--------+\n",
      "|   1|  Alice| 25|New York|\n",
      "|NULL|    Bob| 30|  London|\n",
      "|   3|Charlie| 22|   Paris|\n",
      "|   4|  David| 28|   Tokyo|\n",
      "+----+-------+---+--------+\n",
      "\n",
      "+----+-----+---+--------+-------+\n",
      "|  id| name|age|    city|  local|\n",
      "+----+-----+---+--------+-------+\n",
      "|   1|Alice| 24|New York| monday|\n",
      "|   5|  Eve| 24|  Sydney|   NULL|\n",
      "|   1|  Bob| 35|  London|Tuesday|\n",
      "|NULL|Frank| 35|  Berlin|   NULL|\n",
      "|NULL|  kil| 35|  Berlin| sunday|\n",
      "+----+-----+---+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"JoinExamples\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "\n",
    "schema2 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"local\", StringType(), True)\n",
    "])\n",
    "\n",
    "data1 = [(1, \"Alice\", 25, \"New York\"), (None, \"Bob\", 30, \"London\"), (3, \"Charlie\", 22, \"Paris\"), (4, \"David\", 28, \"Tokyo\")]\n",
    "df1 = spark.createDataFrame(data1, schema=schema)\n",
    "\n",
    "data2 = [(1, \"Alice\", 24, \"New York\",\"monday\"), (5, \"Eve\", 24, \"Sydney\",None), (1, \"Bob\", 35, \"London\",\"Tuesday\"), (None, \"Frank\", 35, \"Berlin\",None),(None, \"kil\", 35, \"Berlin\", \"sunday\")]  # Note: Some overlap in 'id' and 'name'\n",
    "df2 = spark.createDataFrame(data2, schema=schema2)\n",
    "\n",
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+--------+-------+\n",
      "|  id|   name|age|    city|  local|\n",
      "+----+-------+---+--------+-------+\n",
      "|   1|  Alice| 25|New York|   NULL|\n",
      "|NULL|    Bob| 30|  London|   NULL|\n",
      "|   3|Charlie| 22|   Paris|   NULL|\n",
      "|   4|  David| 28|   Tokyo|   NULL|\n",
      "|   1|  Alice| 24|New York| monday|\n",
      "|   5|    Eve| 24|  Sydney|   NULL|\n",
      "|   1|    Bob| 35|  London|Tuesday|\n",
      "|NULL|  Frank| 35|  Berlin|   NULL|\n",
      "|NULL|    kil| 35|  Berlin| sunday|\n",
      "+----+-------+---+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+--------+------+\n",
      "|  id|   name|age|    city| local|\n",
      "+----+-------+---+--------+------+\n",
      "|NULL|    Bob| 30|  London|  NULL|\n",
      "|   1|  Alice| 24|New York|monday|\n",
      "|   3|Charlie| 22|   Paris|  NULL|\n",
      "|   4|  David| 28|   Tokyo|  NULL|\n",
      "|   5|    Eve| 24|  Sydney|  NULL|\n",
      "+----+-------+---+--------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=============================>                             (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+--------+------+\n",
      "|  id|   name|age|    city| local|\n",
      "+----+-------+---+--------+------+\n",
      "|NULL|    Bob| 30|  London|  NULL|\n",
      "|   1|  Alice| 24|New York|monday|\n",
      "|   3|Charlie| 22|   Paris|  NULL|\n",
      "|   4|  David| 28|   Tokyo|  NULL|\n",
      "|   5|    Eve| 24|  Sydney|  NULL|\n",
      "+----+-------+---+--------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# left join\n",
    "df3 = df1.unionByName(df2, allowMissingColumns=True)\n",
    "df3.show()\n",
    "\n",
    "df4 = df3.dropDuplicates([\"id\"]).orderBy([\"id\"])\n",
    "df4.show()\n",
    "\n",
    "df5 = df4.distinct().orderBy([\"id\"])\n",
    "df5.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+--------+----+-----+----+--------+-------+\n",
      "|  id|   name|age|    city|  id| name| age|    city|  local|\n",
      "+----+-------+---+--------+----+-----+----+--------+-------+\n",
      "|   1|  Alice| 25|New York|   1|  Bob|  35|  London|Tuesday|\n",
      "|   1|  Alice| 25|New York|   1|Alice|  24|New York| monday|\n",
      "|NULL|    Bob| 30|  London|NULL|  kil|  35|  Berlin| sunday|\n",
      "|NULL|    Bob| 30|  London|NULL|Frank|  35|  Berlin|   NULL|\n",
      "|   4|  David| 28|   Tokyo|NULL| NULL|NULL|    NULL|   NULL|\n",
      "|   3|Charlie| 22|   Paris|NULL| NULL|NULL|    NULL|   NULL|\n",
      "+----+-------+---+--------+----+-----+----+--------+-------+\n",
      "\n",
      "+----+-------+---+--------+-------+\n",
      "|  id|   name|age|    city|  local|\n",
      "+----+-------+---+--------+-------+\n",
      "|   1|  Alice| 25|New York|   NULL|\n",
      "|NULL|    Bob| 30|  London|   NULL|\n",
      "|   3|Charlie| 22|   Paris|   NULL|\n",
      "|   4|  David| 28|   Tokyo|   NULL|\n",
      "|   1|  Alice| 24|New York| monday|\n",
      "|   5|    Eve| 24|  Sydney|   NULL|\n",
      "|   1|    Bob| 35|  London|Tuesday|\n",
      "|NULL|  Frank| 35|  Berlin|   NULL|\n",
      "|NULL|    kil| 35|  Berlin| sunday|\n",
      "+----+-------+---+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# df3.show()\n",
    "\n",
    "df7 = df1.join(df2, df1.id.eqNullSafe(df2.id), 'left')\n",
    "df7.show()\n",
    "\n",
    "#df8 = df1.union(df2)\n",
    "#df8.show()\n",
    "\n",
    "df9 = df1.unionByName(df2,allowMissingColumns=True)\n",
    "df9.show()\n",
    "# right join\n",
    " \n",
    "df4 = df2.join(df1, df1.id == df2.id, 'right')\n",
    "# df4.show()\n",
    "\n",
    "# inner join \n",
    "df5 = df1.join(df2, df1.id == df2.id, 'inner' )\n",
    "# df5.show()\n",
    "\n",
    "df6 = df1.join(df2, df1.id == df2.id, 'outer')\n",
    "# df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/13 11:06:18 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+-----------+\n",
      "|employee_id|employee_name|age| department|\n",
      "+-----------+-------------+---+-----------+\n",
      "|          1|     John Doe| 30|Engineering|\n",
      "|          2|   Jane Smith| 25|  Marketing|\n",
      "|          3|    Sam Brown| 35|      Sales|\n",
      "+-----------+-------------+---+-----------+\n",
      "\n",
      "+-----------+-----------+-----------+-----+\n",
      "|employee_id|    address|       city|state|\n",
      "+-----------+-----------+-----------+-----+\n",
      "|          1|123 Main St|Springfield|   IL|\n",
      "|          2| 456 Elm St|  Riverside|   CA|\n",
      "|          3| 789 Oak St|    Madison|   WI|\n",
      "+-----------+-----------+-----------+-----+\n",
      "\n",
      "+-----------+-------------+---+-----------+-----------+-----------+-----------+-----+\n",
      "|employee_id|employee_name|age| department|employee_id|    address|       city|state|\n",
      "+-----------+-------------+---+-----------+-----------+-----------+-----------+-----+\n",
      "|          1|     John Doe| 30|Engineering|          1|123 Main St|Springfield|   IL|\n",
      "|          2|   Jane Smith| 25|  Marketing|          2| 456 Elm St|  Riverside|   CA|\n",
      "|          3|    Sam Brown| 35|      Sales|          3| 789 Oak St|    Madison|   WI|\n",
      "+-----------+-------------+---+-----------+-----------+-----------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"Employee Data\").getOrCreate()\n",
    "\n",
    "# Define the schema for employee details DataFrame\n",
    "employee_details_schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), True),\n",
    "    StructField(\"employee_name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"department\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create sample employee details data\n",
    "employee_details_data = [\n",
    "    (1, \"John Doe\", 30, \"Engineering\"),\n",
    "    (2, \"Jane Smith\", 25, \"Marketing\"),\n",
    "    (3, \"Sam Brown\", 35, \"Sales\")\n",
    "]\n",
    "\n",
    "# Create the employee details DataFrame\n",
    "employee_details_df = spark.createDataFrame(employee_details_data, employee_details_schema)\n",
    "\n",
    "# Define the schema for employee address DataFrame\n",
    "employee_address_schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create sample employee address data\n",
    "employee_address_data = [\n",
    "    (1, \"123 Main St\", \"Springfield\", \"IL\"),\n",
    "    (2, \"456 Elm St\", \"Riverside\", \"CA\"),\n",
    "    (3, \"789 Oak St\", \"Madison\", \"WI\")\n",
    "]\n",
    "\n",
    "# Create the employee address DataFrame\n",
    "employee_address_df = spark.createDataFrame(employee_address_data, employee_address_schema)\n",
    "\n",
    "# Show the DataFrames\n",
    "employee_details_df.show()\n",
    "employee_address_df.show()\n",
    "\n",
    "\n",
    "df3 = employee_details_df.join(employee_address_df, employee_details_df.employee_id == employee_address_df.employee_id, 'inner')\n",
    "df3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: delta-spark in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.3.0)\n",
      "Requirement already satisfied: pyspark<3.6.0,>=3.5.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from delta-spark) (3.5.4)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from delta-spark) (8.6.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.21.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pyspark<3.6.0,>=3.5.3->delta-spark) (0.10.9.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "+---+-----+---+--------+\n",
      "| id| name|age|    city|\n",
      "+---+-----+---+--------+\n",
      "|  1|Alice| 25|New York|\n",
      "|  2|  Bob| 30|  London|\n",
      "+---+-----+---+--------+\n",
      "\n",
      "+---+-------+---+-----+-------+\n",
      "| id|   name|age| city|country|\n",
      "+---+-------+---+-----+-------+\n",
      "|  3|Charlie| 22|Paris| France|\n",
      "|  4|  David| 28|Tokyo|  Japan|\n",
      "+---+-------+---+-----+-------+\n",
      "\n",
      "+---+-------+---+--------+-------+\n",
      "| id|   name|age|    city|country|\n",
      "+---+-------+---+--------+-------+\n",
      "|  1|  Alice| 25|New York|   NULL|\n",
      "|  2|    Bob| 30|  London|   NULL|\n",
      "|  3|Charlie| 22|   Paris| France|\n",
      "|  4|  David| 28|   Tokyo|  Japan|\n",
      "+---+-------+---+--------+-------+\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 49\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdelta\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeltaTable\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Create a Delta table\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mDeltaTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateIfNotExists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;241m.\u001b[39mtableName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memployee_details\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;241m.\u001b[39mlocation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/delta/employee_details\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;241m.\u001b[39mdata(employee_details_df) \\\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;241m.\u001b[39mexecute()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/delta/tables.py:479\u001b[0m, in \u001b[0;36mDeltaTable.createIfNotExists\u001b[0;34m(cls, sparkSession)\u001b[0m\n\u001b[1;32m    476\u001b[0m jvm: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJVMView\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m sparkSession\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    477\u001b[0m jsparkSession: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m sparkSession\u001b[38;5;241m.\u001b[39m_jsparkSession  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m jdt \u001b[38;5;241m=\u001b[39m \u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtables\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDeltaTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateIfNotExists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjsparkSession\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DeltaTableBuilder(sparkSession, jdt)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "!pip install delta-spark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Create two DataFrames\n",
    "data1 = [\n",
    "    (1, \"Alice\", 25, \"New York\"), \n",
    "    (2, \"Bob\", 30, \"London\")\n",
    "    ]\n",
    "data2 = [\n",
    "    (3, \"Charlie\", 22, \"Paris\", \"France\"),\n",
    "    (4, \"David\", 28, \"Tokyo\", \"Japan\")\n",
    "    ]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "\n",
    "schema1 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "])\n",
    "\n",
    "df1 = spark.createDataFrame(data1,\n",
    "                             schema=schema)\n",
    "df2 = spark.createDataFrame(data2,\n",
    "                             schema=schema1)\n",
    "\n",
    "df1.show()\n",
    "df2.show()\n",
    "\n",
    "# Perform union\n",
    "df_union = df1.unionByName(df2,\n",
    "                    allowMissingColumns=True)\n",
    "df_union.show()\n",
    "\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Create a Delta table\n",
    "\n",
    "DeltaTable.createIfNotExists(spark) \\\n",
    "    .tableName(\"employee_details\") \\\n",
    "    .location(\"/tmp/delta/employee_details\") \\\n",
    "    .data(employee_details_df) \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: delta-spark in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.3.0)\n",
      "Requirement already satisfied: pyspark<3.6.0,>=3.5.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from delta-spark) (3.5.4)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from delta-spark) (8.6.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.21.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pyspark<3.6.0,>=3.5.3->delta-spark) (0.10.9.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install delta-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdelta\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeltaTable\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create a Delta table\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mDeltaTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateIfNotExists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mtableName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memployee_details\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mlocation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/delta/employee_details\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39maddColumns(employee_details_schema) \\\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39mexecute()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/delta/tables.py:479\u001b[0m, in \u001b[0;36mDeltaTable.createIfNotExists\u001b[0;34m(cls, sparkSession)\u001b[0m\n\u001b[1;32m    476\u001b[0m jvm: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJVMView\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m sparkSession\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    477\u001b[0m jsparkSession: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m sparkSession\u001b[38;5;241m.\u001b[39m_jsparkSession  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m jdt \u001b[38;5;241m=\u001b[39m \u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtables\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDeltaTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateIfNotExists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjsparkSession\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DeltaTableBuilder(sparkSession, jdt)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"JoinExamples\").getOrCreate()\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Create a Delta table\n",
    "DeltaTable.createIfNotExists(spark) \\\n",
    "    .tableName(\"employee_details\") \\\n",
    "    .location(\"/tmp/delta/employee_details\") \\\n",
    "    .addColumns(employee_details_schema) \\\n",
    "    .execute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
