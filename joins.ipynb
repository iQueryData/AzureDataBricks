{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 07:52:15 WARN Utils: Your hostname, codespaces-56288b resolves to a loopback address: 127.0.0.1; using 10.0.2.66 instead (on interface eth0)\n",
      "25/02/02 07:52:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/02 07:52:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/02 07:52:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+--------+\n",
      "|  id|   name|age|    city|\n",
      "+----+-------+---+--------+\n",
      "|   1|  Alice| 25|New York|\n",
      "|NULL|    Bob| 30|  London|\n",
      "|   3|Charlie| 22|   Paris|\n",
      "|   4|  David| 28|   Tokyo|\n",
      "+----+-------+---+--------+\n",
      "\n",
      "+----+-----+---+--------+-------+\n",
      "|  id| name|age|    city|  local|\n",
      "+----+-----+---+--------+-------+\n",
      "|   1|Alice| 24|New York| monday|\n",
      "|   5|  Eve| 24|  Sydney|   NULL|\n",
      "|   1|  Bob| 35|  London|Tuesday|\n",
      "|NULL|Frank| 35|  Berlin|   NULL|\n",
      "|NULL|  kil| 35|  Berlin| sunday|\n",
      "+----+-----+---+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"JoinExamples\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "\n",
    "schema2 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"local\", StringType(), True)\n",
    "])\n",
    "\n",
    "data1 = [(1, \"Alice\", 25, \"New York\"), (None, \"Bob\", 30, \"London\"), (3, \"Charlie\", 22, \"Paris\"), (4, \"David\", 28, \"Tokyo\")]\n",
    "df1 = spark.createDataFrame(data1, schema=schema)\n",
    "\n",
    "data2 = [(1, \"Alice\", 24, \"New York\",\"monday\"), (5, \"Eve\", 24, \"Sydney\",None), (1, \"Bob\", 35, \"London\",\"Tuesday\"), (None, \"Frank\", 35, \"Berlin\",None),(None, \"kil\", 35, \"Berlin\", \"sunday\")]  # Note: Some overlap in 'id' and 'name'\n",
    "df2 = spark.createDataFrame(data2, schema=schema2)\n",
    "\n",
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+--------+----+-----+----+--------+-------+\n",
      "|  id|   name|age|    city|  id| name| age|    city|  local|\n",
      "+----+-------+---+--------+----+-----+----+--------+-------+\n",
      "|NULL|    Bob| 30|  London|NULL| NULL|NULL|    NULL|   NULL|\n",
      "|   1|  Alice| 25|New York|   1|  Bob|  35|  London|Tuesday|\n",
      "|   1|  Alice| 25|New York|   1|Alice|  24|New York| monday|\n",
      "|   3|Charlie| 22|   Paris|NULL| NULL|NULL|    NULL|   NULL|\n",
      "|   4|  David| 28|   Tokyo|NULL| NULL|NULL|    NULL|   NULL|\n",
      "+----+-------+---+--------+----+-----+----+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left join\n",
    "df3 = df1.join(df2, \\\n",
    "df1.id == df2.id, 'left')\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+--------+----+-----+----+--------+-------+\n",
      "|  id|   name|age|    city|  id| name| age|    city|  local|\n",
      "+----+-------+---+--------+----+-----+----+--------+-------+\n",
      "|   1|  Alice| 25|New York|   1|  Bob|  35|  London|Tuesday|\n",
      "|   1|  Alice| 25|New York|   1|Alice|  24|New York| monday|\n",
      "|NULL|    Bob| 30|  London|NULL|  kil|  35|  Berlin| sunday|\n",
      "|NULL|    Bob| 30|  London|NULL|Frank|  35|  Berlin|   NULL|\n",
      "|   4|  David| 28|   Tokyo|NULL| NULL|NULL|    NULL|   NULL|\n",
      "|   3|Charlie| 22|   Paris|NULL| NULL|NULL|    NULL|   NULL|\n",
      "+----+-------+---+--------+----+-----+----+--------+-------+\n",
      "\n",
      "+----+-------+---+--------+-------+\n",
      "|  id|   name|age|    city|  local|\n",
      "+----+-------+---+--------+-------+\n",
      "|   1|  Alice| 25|New York|   NULL|\n",
      "|NULL|    Bob| 30|  London|   NULL|\n",
      "|   3|Charlie| 22|   Paris|   NULL|\n",
      "|   4|  David| 28|   Tokyo|   NULL|\n",
      "|   1|  Alice| 24|New York| monday|\n",
      "|   5|    Eve| 24|  Sydney|   NULL|\n",
      "|   1|    Bob| 35|  London|Tuesday|\n",
      "|NULL|  Frank| 35|  Berlin|   NULL|\n",
      "|NULL|    kil| 35|  Berlin| sunday|\n",
      "+----+-------+---+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 07:52:31 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# df3.show()\n",
    "\n",
    "df7 = df1.join(df2, df1.id.eqNullSafe(df2.id), 'left')\n",
    "df7.show()\n",
    "\n",
    "#df8 = df1.union(df2)\n",
    "#df8.show()\n",
    "\n",
    "df9 = df1.unionByName(df2,allowMissingColumns=True)\n",
    "df9.show()\n",
    "# right join\n",
    " \n",
    "df4 = df2.join(df1, df1.id == df2.id, 'right')\n",
    "# df4.show()\n",
    "\n",
    "# inner join \n",
    "\n",
    "df5 = df1.join(df2, df1.id == df2.id, 'inner' )\n",
    "# df5.show()\n",
    "\n",
    "df6 = df1.join(df2, df1.id == df2.id, 'outer')\n",
    "# df6.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
