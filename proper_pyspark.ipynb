{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/21 17:23:08 WARN Utils: Your hostname, codespaces-56288b resolves to a loopback address: 127.0.0.1; using 10.0.3.12 instead (on interface eth0)\n",
      "25/03/21 17:23:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/21 17:23:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/21 17:23:33 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+--------+\n",
      "|  id|   name|age|    city|\n",
      "+----+-------+---+--------+\n",
      "|   1|  Alice| 25|New York|\n",
      "|NULL|    Bob| 30|  London|\n",
      "|   3|Charlie| 22|   Paris|\n",
      "|   4|  David| 28|   Tokyo|\n",
      "+----+-------+---+--------+\n",
      "\n",
      "+----+-----+---+--------+-------+\n",
      "|  id| name|age|    city|  local|\n",
      "+----+-----+---+--------+-------+\n",
      "|   1|Alice| 24|New York| monday|\n",
      "|   5|  Eve| 24|  Sydney|   NULL|\n",
      "|   1|  Bob| 35|  London|Tuesday|\n",
      "|NULL|Frank| 35|  Berlin|   NULL|\n",
      "|NULL|  kil| 35|  Berlin| sunday|\n",
      "+----+-----+---+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"JoinExamples\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "\n",
    "schema2 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"local\", StringType(), True)\n",
    "])\n",
    "\n",
    "data1 = [(1, \"Alice\", 25, \"New York\"), (None, \"Bob\", 30, \"London\"), (3, \"Charlie\", 22, \"Paris\"), (4, \"David\", 28, \"Tokyo\")]\n",
    "df1 = spark.createDataFrame(data1, schema=schema)\n",
    "\n",
    "data2 = [(1, \"Alice\", 24, \"New York\",\"monday\"), (5, \"Eve\", 24, \"Sydney\",None), (1, \"Bob\", 35, \"London\",\"Tuesday\"), (None, \"Frank\", 35, \"Berlin\",None),(None, \"kil\", 35, \"Berlin\", 'sunday')]  # Note: Some overlap in 'id' and 'name'\n",
    "df2 = spark.createDataFrame(data2, schema=schema2)\n",
    "\n",
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+--------+\n",
      "| id| name|age|    city|\n",
      "+---+-----+---+--------+\n",
      "|  1|Alice| 25|New York|\n",
      "+---+-----+---+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+----+-----+---+--------+-------+\n",
      "|  id| name|age|    city|  id| name|age|    city|  local|\n",
      "+----+-----+---+--------+----+-----+---+--------+-------+\n",
      "|NULL|  Bob| 30|  London|NULL|Frank| 35|  Berlin|   NULL|\n",
      "|NULL|  Bob| 30|  London|NULL|  kil| 35|  Berlin| sunday|\n",
      "|   1|Alice| 25|New York|   1|Alice| 24|New York| monday|\n",
      "|   1|Alice| 25|New York|   1|  Bob| 35|  London|Tuesday|\n",
      "+----+-----+---+--------+----+-----+---+--------+-------+\n",
      "\n",
      "+-------+---------------+\n",
      "|   name|         fruits|\n",
      "+-------+---------------+\n",
      "|   raja|[apple, orange]|\n",
      "| shahid| [banana, kiwi]|\n",
      "|panneer|           NULL|\n",
      "+-------+---------------+\n",
      "\n",
      "+-------+------+\n",
      "|   name|   col|\n",
      "+-------+------+\n",
      "|   raja| apple|\n",
      "|   raja|orange|\n",
      "| shahid|banana|\n",
      "| shahid|  kiwi|\n",
      "|panneer|  NULL|\n",
      "+-------+------+\n",
      "\n",
      "+----+-------+---+--------+-------+\n",
      "|  id|   name|age|    city|country|\n",
      "+----+-------+---+--------+-------+\n",
      "|   1|  Alice| 25|New York|    USA|\n",
      "|NULL|    Bob| 30|  London|     UK|\n",
      "|   3|Charlie| 22|   Paris| France|\n",
      "|   4|  David| 28|   Tokyo|  Japan|\n",
      "+----+-------+---+--------+-------+\n",
      "\n",
      "+----+---------+---+--------+-------+\n",
      "|  id|cust_name|age|    city|country|\n",
      "+----+---------+---+--------+-------+\n",
      "|   1|    Alice| 25|New York|    USA|\n",
      "|NULL|      Bob| 30|  London|     UK|\n",
      "|   3|  Charlie| 22|   Paris| France|\n",
      "|   4|    David| 28|   Tokyo|  Japan|\n",
      "+----+---------+---+--------+-------+\n",
      "\n",
      "+----+---+--------+-------+\n",
      "|  id|age|    city|country|\n",
      "+----+---+--------+-------+\n",
      "|   1| 25|New York|    USA|\n",
      "|NULL| 30|  London|     UK|\n",
      "|   3| 22|   Paris| France|\n",
      "|   4| 28|   Tokyo|  Japan|\n",
      "+----+---+--------+-------+\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/workspaces/AzureDataBricks/Filestore.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m\n\u001b[1;32m     33\u001b[0m df6\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Handling Bad Records in Pyspark \u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# DropMalFormed \u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDROPMALFORMED\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFilestore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/pyspark/sql/readwriter.py:307\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/workspaces/AzureDataBricks/Filestore."
     ]
    }
   ],
   "source": [
    "customer_filter = df1.filter((col('city') == 'New York') & (col('age') <= 30 ))\n",
    "customer_filter.show()\n",
    "\n",
    "inner_join = df1.join(df2, df1.id.eqNullSafe(df2.id), 'inner')\n",
    "inner_join.show()\n",
    "\n",
    "data = [\n",
    "    ('raja', ['apple', 'orange']),\n",
    "    ('shahid', ['banana','kiwi']),\n",
    "    ('panneer', None)\n",
    "]\n",
    "\n",
    "df3 = spark.createDataFrame(data=data, schema=['name','fruits'])\n",
    "df3.show()\n",
    "\n",
    "df4 = df3.select(col('name'), explode_outer(col('fruits')))\n",
    "df4.show()\n",
    "\n",
    "\n",
    "df5 = df1.withColumn(\"country\", when(col(\"city\") == \"Berlin\", \"Russia\")\n",
    "                     .when(col(\"city\") == \"New York\", \"USA\")\n",
    "                     .when(col(\"city\") == \"London\",\"UK\")\n",
    "                     .when(col(\"city\") == \"Paris\", \"France\")\n",
    "                     .when(col(\"city\") == \"Tokyo\", \"Japan\")\n",
    "                     .otherwise(\"Third World Country\"))\n",
    "\n",
    "df5.show()\n",
    "\n",
    "df6 = df5.withColumnRenamed(\"name\", \"cust_name\")\n",
    "df6.show()\n",
    "\n",
    "df6 = df6.drop(col(\"cust_name\"))\n",
    "df6.show()\n",
    "\n",
    "\n",
    "# Handling Bad Records in Pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+--------+\n",
      "|  id|   name|age|    city|\n",
      "+----+-------+---+--------+\n",
      "|   1|  Alice| 25|New York|\n",
      "|NULL|    Bob| 30|  London|\n",
      "|   3|Charlie| 22|   Paris|\n",
      "|   4|  David| 28|   Tokyo|\n",
      "+----+-------+---+--------+\n",
      "\n",
      "+----+-------+---+--------+\n",
      "|  id|   name|age|    city|\n",
      "+----+-------+---+--------+\n",
      "|   1|  Alice| 25|New York|\n",
      "|   4|  David| 28|   Tokyo|\n",
      "|NULL|    Bob| 30|  London|\n",
      "|   3|Charlie| 22|   Paris|\n",
      "+----+-------+---+--------+\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m df2 \u001b[38;5;241m=\u001b[39m df1\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      6\u001b[0m df2\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m----> 8\u001b[0m \u001b[43msc\u001b[49m\u001b[38;5;241m.\u001b[39mdefaultParallelism\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "# performing re-partiton and coalesce \n",
    "\n",
    "df1.show()\n",
    "\n",
    "df2 = df1.repartition(2)\n",
    "df2.show()\n",
    "\n",
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---------+-----+\n",
      "|  id|    name| location|salry|\n",
      "+----+--------+---------+-----+\n",
      "|1001|  gaurav|hyderabad|42000|\n",
      "|1002|   vijay|hyderabad|45565|\n",
      "|1003|akanksha|hyderabad|52000|\n",
      "|1004|niharika|hyderabad|35000|\n",
      "+----+--------+---------+-----+\n",
      "\n",
      "daily data...\n",
      "+----+--------+---------+-----+\n",
      "|  id|    name| location|salry|\n",
      "+----+--------+---------+-----+\n",
      "|1003|akanksha|    delhi|65000|\n",
      "|1004|niharika|    bihar|10000|\n",
      "|1005|  murali|vijaywada|80000|\n",
      "|1002|   vijay|hyderabad|45565|\n",
      "+----+--------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# list of employee data\n",
    "data = [[\"1001\", \"gaurav\", \"hyderabad\",\"42000\"],\n",
    "[\"1002\", \"vijay\", \"hyderabad\",\"45565\"],\n",
    "[\"1003\", \"akanksha\",\"hyderabad\", \"52000\"],\n",
    "[\"1004\", \"niharika\", \"hyderabad\",\"35000\"]]\n",
    "# specify column names\n",
    "columns = ['id','name','location','salry']\n",
    "# creating a dataframe from the lists of data\n",
    "df_full = spark.createDataFrame(data, columns)\n",
    "# list of employee data\n",
    "data = [ [\"1003\", \"akanksha\",\"delhi\", \"65000\"],\n",
    "[\"1004\", \"niharika\", \"bihar\",\"10000\"],\n",
    "[\"1005\", \"murali\",\"vijaywada\", \"80000\"],\n",
    "[\"1002\", \"vijay\", \"hyderabad\",\"45565\"]\n",
    "]\n",
    "# specify column names\n",
    "columns = ['id','name','location','salry']\n",
    "# creating a dataframe from the lists of data\n",
    "df_daily_update = spark.createDataFrame(data, columns)\n",
    "print(\"Full data...\")\n",
    "df_full.show()\n",
    "print(\"daily data...\")\n",
    "df_daily_update.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
